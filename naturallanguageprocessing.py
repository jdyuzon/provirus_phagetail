# -*- coding: utf-8 -*-
"""NaturalLanguageProcessing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FCD4pqoSyeknVP0FiqVUKK82JcoPsp5K
"""

### Natural Language Processing to classify genes as either provirus or phage tail using annotation description
### Recurrent Neural Network (RNN) is commonly used for Natural Language Processing.
### RNN stores information for current features and neighboring features and is useful if prediction is at word-level
### Long Short-Term Memory Cell (LSTM) are similar to RNNs except that hidden layer updates are replaced by memory cells which is useful for long range dependencies such as sentence structures

### scripts adapted from:
### https://www.tensorflow.org/tutorials/keras/text_classification
### https://www.tensorflow.org/tutorials/structured_data/feature_columns
### https://www.analyticsvidhya.com/blog/2020/12/understanding-text-classification-in-nlp-with-movie-review-example-example/
### https://stackoverflow.com/questions/58636087/tensorflow-valueerror-failed-to-convert-a-numpy-array-to-a-tensor-unsupporte

#%pip install absl-py
#%pip install tensorflow --upgrade
#%pip install numpy --upgrade
#%pip install -U scikit-learn

# Commented out IPython magic to ensure Python compatibility.
# %load_ext autoreload
# %autoreload 2

import pandas as pd
import numpy as np
import sys
import os
import re
import string
import shutil

import keras
import tensorflow as tf
from sklearn.model_selection import train_test_split

from tensorflow import feature_column
from tensorflow.keras import layers
from tensorflow.keras import losses
from sklearn.model_selection import train_test_split

from keras.models import Sequential
from tensorflow.keras.layers import LSTM
from tensorflow.keras.layers import Dropout
from tensorflow.keras.layers import Dense

### set work directory
os.chdir("/pscratch/sd/j/jdyuzon/snakemake-bacteriophage2")

### load data for gene annotation descriptions **** Later include bacterial chromosome and plasmid data ****
genes_phage_tail=pd.read_csv("prediction_out/all_genes.phage_tail.bed", sep='\t')
# subset data to include ID, type, gene, annotation description
genes_phage_tail = genes_phage_tail[["ID", "type", "annotationdescription", "gene"]]
# remove NAs
genes_phage_tail=genes_phage_tail.dropna(axis=0, how="any", subset=None, inplace=False)
genes_phage_tail

# some rows are duplicates where genes have the same annotation. drop duplicate rows
genes_phage_tail=genes_phage_tail.drop_duplicates()
genes_phage_tail

# some genes have multiple annotations
gene = genes_phage_tail["gene"]
genes_phage_tail[gene.isin(gene[gene.duplicated()])].sort_values("gene")
# sometimes TIGER mistakes a phage for a phage-tail especially when the phage is low quality (Phage 2)
# 50 instances where a T6SS gene was reported as a Phage2 gene

# remove duplicates that are Phage2 but keep T6SS since these are experimentally verified
phage2dups=genes_phage_tail[gene.isin(gene[gene.duplicated()]) & (genes_phage_tail['ID']=='Phage2')].index
genes_phage_tail=genes_phage_tail.drop(phage2dups)
genes_phage_tail

### Looking at the distribution of the data, it is highly imbalanced
genes_phage_tail['type'].value_counts()

### There are 1883 unique annotation descriptions
genes_phage_tail['annotationdescription'].value_counts()

### Create a target variable: the label types 1 and 0 correspond to provirus and phage tail
y=genes_phage_tail['type']
y=y.replace('Phage1',1)
y=y.replace('Phage2',1)
y=y.replace('T6SS',0)
y=y.replace('eCIS',0)
genes_phage_tail['target']=y
#y = int(y)
### drop un-used columns
genes_phage_tail = genes_phage_tail.drop(columns=['ID', 'type','gene'])
genes_phage_tail

genes_phage_tail['target'].value_counts()

### split the data to train, validation and test
train, test = train_test_split(genes_phage_tail, test_size=0.2)
train, val = train_test_split(train, test_size=0.2)
print(len(train), 'train examples')
print(len(val), 'validation examples')
print(len(test), 'test examples')

### the TextVectorization layer calls the standardization function, tokenizes the data, and vectorize (converts sequences to numbers)
### Vectorize converts tokenized data into number so that they can be input into a neural network
max_features = 10000
sequence_length = 1000

vectorize_layer = layers.TextVectorization(
    standardize='lower_and_strip_punctuation',
    split='whitespace',
    max_tokens=max_features,
    output_mode='int',
    output_sequence_length=sequence_length)

### Make a text-only dataset (without labels), then call adapt
### vocabulary for the layer must be either supplied on construction or learned via adapt()
### Then this layer is adapted, it will analyze the dataset, determine the frequency of individual string values, and create a vocabulary from them
vectorize_layer.adapt(train['annotationdescription'])

### Preprocess after train validation test split to prevent leakage
def vectorize_text(text, label):
  text = tf.expand_dims(text, -1)
  return vectorize_layer(text), label

### Double check the dataset
def show_shapes(): # can make yours to take inputs; this'll use local variable values
    print("Expected: (num_samples, timesteps, channels)")
    print("Sequences: {}".format(Sequences.shape))
    print("Targets:   {}".format(Targets.shape))


def get_data(sequences, target):
    #Sequences = np.asarray(vectorize_layer(train['annotationdescription']))
    #Targets   = np.asarray(train['target'])
    Sequences = np.asarray(vectorize_layer(sequences))
    Targets   = np.asarray(target)
    show_shapes()

    Sequences = np.expand_dims(Sequences, -1)
    Targets   = np.expand_dims(Targets, -1)
    show_shapes()
    return Sequences, Targets

### Preprocess on individual datasets to prevent leakage
train_x,train_y=get_data(train['annotationdescription'],train['target'])
val_x,val_y=get_data(val['annotationdescription'],val['target'])
test_x,test_y=get_data(test['annotationdescription'],test['target'])

model = Sequential()

model.add(LSTM(128, activation='tanh',recurrent_activation='sigmoid',
               input_shape=(1000, 1),return_sequences=True))
model.add(Dropout(0.2)) # Dropout layer help prevent overfitting by randomly setting input units to 0 with the specified rate
model.add(LSTM(128, activation='tanh',recurrent_activation='sigmoid')) # 128 units: number of units is the dimension of the hidden state (or the output); defines the dimension of hidden states (or outputs) and the number of params
model.add(Dropout(0.2))
model.add(Dense(32, activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(1, activation='sigmoid'))

#opt = tf.keras.optimizers.Adam(lr=1e-3)

model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])

model.fit(train_x,train_y, epochs=10, validation_data=(val_x,val_y))

loss, accuracy = model.evaluate(test_x,test_y)



